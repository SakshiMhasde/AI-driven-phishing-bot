{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SakshiMhasde/AI-driven-phishing-bot/blob/main/dorkingphishy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Function to extract emails from a web page and ensure no duplicates\n",
        "def extract_emails(page_content):\n",
        "    # Regex pattern for matching emails\n",
        "    email_pattern = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+'\n",
        "    # Find all emails using regex and return as a list of unique emails (set)\n",
        "    return re.findall(email_pattern, page_content)\n",
        "\n",
        "# Function to fetch a webpage content\n",
        "def fetch_page(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            return response.text\n",
        "        else:\n",
        "            return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to get all Google Dorked pages for the given website\n",
        "def get_indexed_pages(website):\n",
        "    query = f'site:{website}'\n",
        "    google_search_url = f\"https://www.google.com/search?q={query}&num=100\"\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    # Fetch the Google search results page\n",
        "    search_results = fetch_page(google_search_url)\n",
        "\n",
        "    if search_results:\n",
        "        soup = BeautifulSoup(search_results, 'html.parser')\n",
        "        # Extract links to the indexed pages\n",
        "        links = []\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            if website in href and 'webcache' not in href:\n",
        "                # Extract URL from the 'href' attribute (clean it)\n",
        "                actual_link = href.split('&')[0].replace(\"/url?q=\", \"\")\n",
        "                links.append(actual_link)\n",
        "        return links\n",
        "    return []\n",
        "\n",
        "# Main function to extract emails from all indexed pages\n",
        "def extract_emails_from_site(website):\n",
        "    print(f\"Finding indexed pages for: {website}\")\n",
        "    pages = get_indexed_pages(website)\n",
        "\n",
        "    all_emails = set()  # To store unique emails\n",
        "    email_data = []     # To store emails along with the page source\n",
        "    sr_no = 1           # Serial number starting from 1\n",
        "    if pages:\n",
        "        print(f\"Found {len(pages)} indexed pages. Extracting emails...\")\n",
        "        for page in pages:\n",
        "            content = fetch_page(page)\n",
        "            if content:\n",
        "                emails = extract_emails(content)\n",
        "                for email in emails:\n",
        "                    if email not in all_emails:  # Only add new emails\n",
        "                        all_emails.add(email)    # Ensure it's unique\n",
        "                        email_data.append({'Sr No': sr_no, 'Email': email, 'Page URL': page})\n",
        "                        sr_no += 1               # Increment the serial number\n",
        "    else:\n",
        "        print(\"No indexed pages found.\")\n",
        "\n",
        "    return all_emails, email_data\n",
        "\n",
        "# Input: Website to scrape\n",
        "website = input(\"Enter the website (e.g., vitbhopal.ac.in): \").strip()\n",
        "\n",
        "# Extract emails from the given website\n",
        "emails, email_data = extract_emails_from_site(website)\n",
        "\n",
        "if emails:\n",
        "    # Save the emails to an Excel file with sequential numbering\n",
        "    df = pd.DataFrame(email_data)\n",
        "    output_file = \"extracted_emails_sequential.xlsx\"\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"\\nEmails have been saved to {output_file}\")\n",
        "else:\n",
        "    print(\"No emails found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JIMnpWRvma-",
        "outputId": "e509d3c1-81e1-4210-cf62-f7d385dacdb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the website (e.g., vitbhopal.ac.in): vitbhopal.ac.in\n",
            "Finding indexed pages for: vitbhopal.ac.in\n",
            "Found 116 indexed pages. Extracting emails...\n",
            "Error fetching /search?q=site:vitbhopal.ac.in: No connection adapters were found for '/search?q=site:vitbhopal.ac.in'\n",
            "Error fetching /search?q=site:vitbhopal.ac.in: No connection adapters were found for '/search?q=site:vitbhopal.ac.in'\n",
            "Error fetching /search%3Fq%3Dsite:vitbhopal.ac.in%26num%3D100%26sca_esv%3D923792fc01f6d6a7%26ie%3DUTF-8%26tbm%3Dshop%26source%3Dlnms%26ved%3D1t:200713%26ictx%3D111: No connection adapters were found for '/search%3Fq%3Dsite:vitbhopal.ac.in%26num%3D100%26sca_esv%3D923792fc01f6d6a7%26ie%3DUTF-8%26tbm%3Dshop%26source%3Dlnms%26ved%3D1t:200713%26ictx%3D111'\n",
            "Error fetching /search?q=site:vitbhopal.ac.in: No connection adapters were found for '/search?q=site:vitbhopal.ac.in'\n",
            "Error fetching /search?q=site:vitbhopal.ac.in: No connection adapters were found for '/search?q=site:vitbhopal.ac.in'\n",
            "Error fetching /search?q=site:vitbhopal.ac.in: No connection adapters were found for '/search?q=site:vitbhopal.ac.in'\n",
            "Error fetching /search?q=site:vitbhopal.ac.in: No connection adapters were found for '/search?q=site:vitbhopal.ac.in'\n",
            "Error fetching /search?q=site:vitbhopal.ac.in: No connection adapters were found for '/search?q=site:vitbhopal.ac.in'\n",
            "Error fetching /search?q=site:vitbhopal.ac.in: No connection adapters were found for '/search?q=site:vitbhopal.ac.in'\n",
            "Error fetching /search?q=site:vitbhopal.ac.in: No connection adapters were found for '/search?q=site:vitbhopal.ac.in'\n",
            "Error fetching /search?q=site:vitbhopal.ac.in: No connection adapters were found for '/search?q=site:vitbhopal.ac.in'\n",
            "Error fetching /search?q=site:vitbhopal.ac.in: No connection adapters were found for '/search?q=site:vitbhopal.ac.in'\n",
            "Error fetching /search?q=site:vitbhopal.ac.in: No connection adapters were found for '/search?q=site:vitbhopal.ac.in'\n",
            "Error fetching /setprefs?hl=en: Invalid URL '/setprefs?hl=en': No scheme supplied. Perhaps you meant https:///setprefs?hl=en?\n",
            "\n",
            "Emails have been saved to extracted_emails_sequential.xlsx\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcK3wjpM4Vg5AZTKcvXx9T",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}